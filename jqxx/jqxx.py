import streamlit as st
import joblib
import numpy as np
import os
import re
import jieba
import pandas as pd
import json
from datetime import datetime

# åˆå§‹åŒ–è®¾ç½®
st.set_page_config(page_title="å‡æ–°é—»æ£€æµ‹ç³»ç»Ÿ", page_icon="ğŸ”", layout="wide")

# å¯åŠ¨éªŒè¯
def check_launch():
    """æ˜¾ç¤ºå¯åŠ¨ä¿¡æ¯"""
    st.title("ğŸ” ä¸“ä¸šå‡æ–°é—»æ£€æµ‹ç³»ç»Ÿ")
    st.markdown("ä½¿ç”¨å…ˆè¿›AIæŠ€æœ¯åˆ†ææ–°é—»çœŸå®æ€§")
    st.divider()
    st.caption(f"ç³»ç»Ÿç‰ˆæœ¬: 3.0 | æ›´æ–°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d')}")

# æ¨¡å‹åŠ è½½å‡½æ•°
@st.cache_resource
def load_models():
    """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ç»„ä»¶"""
    models = {}
    model_dir = "models"
    
    try:
        models["tfidf"] = joblib.load(os.path.join(model_dir, "tfidf_vectorizer_enhanced.pkl"))
        models["classifier"] = joblib.load(os.path.join(model_dir, "enhanced_fake_news_model.pkl"))
        models["expected_dim"] = 204
        return models
    except Exception as e:
        st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        st.stop()

# æ–‡æœ¬é¢„å¤„ç†å‡½æ•°
def preprocess_text(text):
    """å®‰å…¨é¢„å¤„ç†æ–‡æœ¬"""
    if not text: return ""
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9]', ' ', text)
    text = re.sub(r'\s+', ' ', text).lower()
    return " ".join(jieba.cut(text.strip()))

# å®‰å…¨ç‰¹å¾å·¥ç¨‹
def extract_text_features(text):
    """æå–å®‰å…¨ç‰¹å¾ - æ— è¿­ä»£é—®é¢˜"""
    return {
        # åŸºæœ¬ç‰¹å¾
        'length': len(text),
        'sentences': text.count('ã€‚') + 1,
        
        # å¯ä¿¡åº¦ä¿¡å·
        'reliable': 1.0 if 'æ–°åç¤¾' in text or 'äººæ°‘æ—¥æŠ¥' in text or 'å®˜æ–¹' in text else 0.0,
        
        # è’è°¬å†…å®¹ç›´æ¥è¯†åˆ«
        'is_absurd': 1.0 if any(phrase in text for phrase in 
                                ['æœˆçƒçˆ†ç‚¸', 'å¤ªé˜³æ¶ˆå¤±', 'åœ°çƒåœè½¬', 'é•¿ç”Ÿä¸è€']) else 0.0,
        
        # å¼‚å¸¸ç‰¹å¾
        'urgent': 1.0 if 'å¿…çœ‹' in text or 'ç´§æ€¥' in text or 'é€Ÿçœ‹' in text else 0.0,
        'exaggeration': 1.0 if 'éœ‡æƒŠ' in text or 'æœ€ç‰›' in text or '100%' in text else 0.0
    }

# ç‰¹å¾ç”Ÿæˆå‡½æ•°ï¼ˆä¿®å¤ç‰ˆæœ¬ï¼‰
def generate_features(text, models):
    """ç”Ÿæˆç‰¹å¾å¹¶ç¡®ä¿å®‰å…¨ç±»å‹"""
    processed_text = preprocess_text(text)
    
    # å…³é”®ä¿®å¤ï¼šç‰¹å¾å‘é‡æ˜ç¡®ç±»å‹è½¬æ¢
    try:
        # TF-IDFç‰¹å¾
        tfidf_vector = models["tfidf"].transform([processed_text]).toarray()
        
        # é«˜çº§ç‰¹å¾æå–
        adv_features = extract_text_features(text)
        adv_vector = [float(adv_features[key]) for key in sorted(adv_features)]
        
        # åˆå¹¶ç‰¹å¾å¹¶ç¡®ä¿ç»´åº¦
        features = np.concatenate([tfidf_vector[0], adv_vector])
        
        # ä¸¥æ ¼ç±»å‹è½¬æ¢ç¡®ä¿å®‰å…¨
        return features[:models["expected_dim"]].astype(np.float32).reshape(1, -1)
    
    except Exception as e:
        st.error(f"ç‰¹å¾ç”Ÿæˆé”™è¯¯: {str(e)}")
        return np.zeros((1, models["expected_dim"]), dtype=np.float32)

# ä¸“ä¸šè’è°¬å†…å®¹è¯†åˆ«ç³»ç»Ÿ
def absurd_content_detector(text):
    """ç›´æ¥è¯†åˆ«è’è°¬å†…å®¹ - æ— éœ€è¿­ä»£"""
    absurd_phrases = ['æœˆçƒçˆ†ç‚¸', 'å¤ªé˜³æ¶ˆå¤±', 'åœ°çƒåœè½¬', 'é•¿ç”Ÿä¸è€', 'ç©¿è¶Šå¤ä»£']
    for phrase in absurd_phrases:
        if phrase in text:
            return True, f"æ£€æµ‹åˆ°è’è°¬å†…å®¹: {phrase}"
    return False, ""

# ç½®ä¿¡åº¦è®¡ç®—
def calculate_confidence(probabilities, text):
    """å®‰å…¨ç½®ä¿¡åº¦è®¡ç®—"""
    real_prob, fake_prob = probabilities
    
    # æƒå¨æ¥æºæ˜¾è‘—æå‡çœŸå®æ¦‚ç‡
    if 'æ–°åç¤¾' in text or 'äººæ°‘æ—¥æŠ¥' in text:
        real_prob = min(real_prob * 1.7, 0.97)
    
    return real_prob, fake_prob

# ç”¨æˆ·ç•Œé¢ä¸»å‡½æ•°
def main_application():
    # åˆå§‹åŒ–
    if "show_feedback" not in st.session_state:
        st.session_state.show_feedback = False
    
    # ä¾§è¾¹æ 
    with st.sidebar:
        st.subheader("ç³»ç»ŸçŠ¶æ€")
        models = load_models()
        st.success("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
        st.info(f"ç‰¹å¾ç»´åº¦: {models['expected_dim']}")
        
        st.divider()
        st.subheader("ä½¿ç”¨æŒ‡å—")
        st.markdown("""
        1. ç²˜è´´å®Œæ•´æ–°é—»å†…å®¹
        2. é¿å…å•å¥æ£€æµ‹
        3. æƒå¨æ¥æºæé«˜å¯ä¿¡åº¦
        4. å¤¸å¼ è¯æ±‡å¯èƒ½å¯¼è‡´è¯¯åˆ¤
        """)
        
        st.divider()
        if st.button("æŠ¥å‘Šåˆ†æé”™è¯¯", use_container_width=True):
            st.session_state.show_feedback = True

    # ä¸»ç•Œé¢
    st.title("æ–°é—»çœŸå®æ€§æ£€æµ‹")
    news_text = st.text_area("æ–°é—»å†…å®¹:", height=200, 
                            placeholder="ç²˜è´´æ–°é—»å†…å®¹...",
                            help="æ”¯æŒä¸­è‹±æ–‡å†…å®¹",
                            key="news_input")
    
    if st.button("æ£€æµ‹çœŸå®æ€§", type="primary", use_container_width=True):
        if not news_text.strip():
            st.warning("è¯·è¾“å…¥æ–°é—»å†…å®¹")
            return
            
        with st.spinner("æ·±åº¦åˆ†æä¸­..."):
            try:
                # æ­¥éª¤1: è’è°¬å†…å®¹ç›´æ¥è¯†åˆ«
                is_absurd, reason = absurd_content_detector(news_text)
                if is_absurd:
                    st.error(f"âš ï¸ è™šå‡æ–°é—» - {reason}")
                    st.progress(1.0, "è™šå‡ç¨‹åº¦: 100%")
                    
                    with st.expander("è¯¦ç»†åˆ†ææŠ¥å‘Š", expanded=True):
                        st.markdown("### å†…å®¹åˆ†æ")
                        st.warning(f"ç³»ç»Ÿç›´æ¥è¯†åˆ«åˆ°å…³é”®è’è°¬çŸ­è¯­: **{reason.split(':')[-1]}**")
                        st.markdown("### å»ºè®®éªŒè¯")
                        st.info("æ­¤ç±»å†…å®¹é€šå¸¸ç¼ºä¹ç§‘å­¦ä¾æ®ï¼Œå»ºè®®é€šè¿‡å®˜æ–¹æ¸ é“æ ¸å®")
                    return
                
                # æ­¥éª¤2: æ¨¡å‹åˆ†æ
                features = generate_features(news_text, models)
                prediction = models["classifier"].predict(features)[0]
                probabilities = models["classifier"].predict_proba(features)[0]
                real_prob, fake_prob = calculate_confidence(probabilities, news_text)
                
                # å½’ä¸€åŒ–å¤„ç†
                total = real_prob + fake_prob
                real_prob, fake_prob = real_prob/total, fake_prob/total
                
                # æ­¥éª¤3: æ˜¾ç¤ºç»“æœ
                st.subheader("æ£€æµ‹ç»“æœ")
                if prediction == 1:  # å‡æ–°é—»
                    st.error(f"âš ï¸ è™šå‡æ–°é—» (ç½®ä¿¡åº¦: {fake_prob*100:.1f}%)")
                    st.progress(fake_prob, f"è™šå‡ç¨‹åº¦: {fake_prob*100:.1f}%")
                else:  # çœŸæ–°é—»
                    st.success(f"âœ… çœŸå®æ–°é—» (ç½®ä¿¡åº¦: {real_prob*100:.1f}%)")
                    st.progress(real_prob, f"å¯ä¿¡ç¨‹åº¦: {real_prob*100:.1f}%")
                
                # æ­¥éª¤4: è¯¦ç»†æŠ¥å‘Š
                with st.expander("ğŸ“Š è¯¦ç»†åˆ†ææŠ¥å‘Š", expanded=True):
                    features = extract_text_features(news_text)
                    st.markdown("### å†…å®¹ç‰¹å¾åˆ†æ")
                    
                    col1, col2 = st.columns(2)
                    col1.metric("å†…å®¹é•¿åº¦", f"{features['length']} å­—")
                    col2.metric("ç§‘å­¦å¯ä¿¡åº¦", "ä½" if features['is_absurd'] else "é«˜", 
                             delta=None if features['is_absurd'] else "+å¯é ")
                    
                    # å…³é”®ç‰¹å¾æŒ‡æ ‡
                    st.markdown("#### å¯ä¿¡åº¦æŒ‡æ ‡")
                    if features['reliable']:
                        st.success("âœ… åŒ…å«æƒå¨æ¥æºæˆ–å®˜æ–¹å£°æ˜")
                    else:
                        st.info("â„¹ï¸ æœªæ£€æµ‹åˆ°æƒå¨æ¥æºå¼•ç”¨")
                    
                    if features['urgent'] or features['exaggeration']:
                        st.warning("âš ï¸ æ£€æµ‹åˆ°å¯èƒ½å¤¸å¤§è¯æ±‡")
                    
                # ç”¨æˆ·åé¦ˆ
                st.session_state.last_result = {
                    "text": news_text, 
                    "prediction": prediction,
                    "confidence": fake_prob if prediction == 1 else real_prob
                }
                
            except Exception as e:
                st.error(f"åˆ†æå¤±è´¥: {str(e)}")
                # æä¾›è¯¦ç»†é”™è¯¯ä¿¡æ¯
                import traceback
                st.code(traceback.format_exc())

    # ç”¨æˆ·åé¦ˆç³»ç»Ÿ
    if st.session_state.get("show_feedback", False):
        st.divider()
        st.subheader("âœï¸ æŠ¥å‘Šåˆ†æé”™è¯¯")
        
        actual_label = st.radio("æ–°é—»å®é™…ç±»å‹:", ("çœŸå®", "è™šå‡"))
        st.text_area("è¡¥å……è¯´æ˜ (å¯é€‰):", key="feedback_comment")
        
        if st.button("æäº¤åé¦ˆ"):
            # ä¿å­˜åé¦ˆä¿¡æ¯
            feedback_dir = "user_feedback"
            os.makedirs(feedback_dir, exist_ok=True)
            
            feedback_data = {
                "timestamp": datetime.now().isoformat(),
                "text": st.session_state.get("last_result", {}).get("text", "")[:500],
                "reported_label": "real" if actual_label == "çœŸå®" else "fake",
                "comment": st.session_state.feedback_comment
            }
            
            try:
                with open(os.path.join(feedback_dir, "feedback.json"), "a") as f:
                    f.write(json.dumps(feedback_data, ensure_ascii=False) + "\n")
                st.success("æ„Ÿè°¢æ‚¨çš„åé¦ˆï¼ç³»ç»Ÿå°†æ®æ­¤æ”¹è¿›")
                st.session_state.show_feedback = False
            except Exception as e:
                st.error(f"åé¦ˆä¿å­˜å¤±è´¥: {str(e)}")

# ä¸»ç¨‹åºå…¥å£
if __name__ == "__main__":
    check_launch()
    main_application()
